# ğŸ—‚ï¸ Data Lake Application  

## ğŸ“– Project Description  
This project demonstrates the creation of a scalable and efficient **Data Lake** using AWS services. The application is designed to ingest, store, process, and analyze large volumes of data, enabling real-time insights and analytics.  

With this project, I explored event-driven architecture and serverless computing, leveraging AWS tools to build a robust solution for managing data streams effectively.  

---

## ğŸ› ï¸ Features  
- **Data Ingestion**: Stream real-time data into the data lake using AWS Kinesis.  
- **ETL Processing**: Transform raw data into structured formats using AWS Glue.  
- **Data Storage**: Store raw and processed data securely in AWS S3.  
- **Query and Analysis**: Perform SQL-like queries on the data lake using AWS Athena.  
- **Scalability**: Built to handle large-scale data with minimal operational overhead.  

---

## ğŸš€ Steps to Set Up the Data Lake  

### 1. **Prerequisites**  
Before starting, ensure you have the following:  
- AWS account with required permissions for S3, Kinesis, Glue, and Athena.  
- AWS CLI installed and configured (`aws configure`).  
- Python 3.8+ installed on your local machine.  

### 2. **Clone the Repository**  
Clone this repository to your local machine:  
```bash
git clone https://github.com/CloudDev-x/30_Days_DevOps_Challenge.git
cd DAY_3_DATA_LAKE